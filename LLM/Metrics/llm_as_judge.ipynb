{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f95081ad-34c8-4196-81aa-795fba524cca",
   "metadata": {},
   "source": [
    "### LLM as a Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc9a9f-e2c8-4049-989f-e5d20e72a86b",
   "metadata": {},
   "source": [
    "As métricas apresentadas até agora tem uma coisa em comum: todas elas são exatas, no sentido em que contabilizam a existência de tokens/palavras no texto avaliado para obter um score. Mesmo no caso dos testes baseados em MMLU, existe uma opção válida (a,b,c,d,etc), o que facilita uma validação.\n",
    "O que dizer então de respostas abertas, muito comuns em chatbots por exemplo?\n",
    "Neste caso, entra um conceito chamado de Judging LLM-as-a-Judge.\n",
    "Em se tratando de assuntos subjetivos, a idéia é utilizar um modelo superior para atuar como \"árbitro\", como GPT-4 por exemplo.\n",
    "Além disso, existe um benchmark chamado MT-BENCH, que consiste de questões abertas criadas para avaliar a capacidade conversacional e habilidade para seguir instruções. E claro, um processo de criação dessas questões e respostas, utilizando humanos, é ideal para avaliar preferências, porém muito trabalhoso e lento. \n",
    "Uma outro possibilidade também é utilizar the Judge Model para avaliar as respostas, e coletar também um feedback humano sobre a decisão. Seria uma maneira de medir o quanto existe de concordância entre o humano e o modelo agindo como Juiz.\n",
    "O assunto vai mais longe ainda, se considerarmos que pode existir viés, embutido tanto nas respostas do modelo, quanto nas respostas de um humano. \n",
    "Existem diversas formas de executar uma avaliação:\n",
    "Comparação de pares de respostas: qual a melhor resposta, dada a pergunta xyz e um determinado critério\n",
    "Avaliação de uma resposta, baseada em um critério\n",
    "Avaliação baseada em referências: a resposta é avaliada com base em critérios e também com base em outras referências, como por exemplo, conteúdo proveniente de RAG\n",
    "\n",
    "Entre as vantagens de um LLM como Juiz, podemos considerar a escabilidade, que se traduz na redução do envolvimento humano no processo (apesar que ele deverá se envolver em alguma medida, ou você delegaria todo o processo para o LLM?) e a explicabilidade, que realmente é muito importante, em se tratando de assuntos subjetivos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "728c9322-9dd7-4e65-93a2-063a00226507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install langchain_community\n",
    "#! pip install oci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3be36f3d-2ae3-49b3-873e-e6d7e4eee8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oci\n",
    "import json\n",
    "\n",
    "#Bibliotecas para uso do langchain + OCI Generative AI\n",
    "from langchain_community.chat_models import ChatOCIGenAI\n",
    "from langchain_community.chat_models import oci_generative_ai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d483ed-6435-452f-9832-830896b291a8",
   "metadata": {},
   "source": [
    "### Autenticação OCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "367c4de2-96b2-4fc5-9b85-477df6e95220",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARTMENT_ID = \"ocid1.compartment.oc1..aaaaaaaacuafyhpnjnsp5luoo3dqklsdi2ysobswq3irzu664gl3cjhvcjpa\"\n",
    "AUTH_TYPE = \"API_KEY\" # The authentication type to use, e.g., API_KEY (default), SECURITY_TOKEN, INSTANCE_PRINCIPAL, RESOURCE_PRINCIPAL.\n",
    "\n",
    "# Auth Config\n",
    "CONFIG_PROFILE = \"DEFAULT\"\n",
    "config = oci.config.from_file('config', CONFIG_PROFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c07ca6-6f0c-45eb-ab3a-89212bbae422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73b0fd93-b069-4397-b921-0e02ba4976ab",
   "metadata": {},
   "source": [
    "### Instância OCI Generative AI com modelo gpt 4.1 nano, executado dentro do OCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "085e41c1-d414-4924-89b1-ecaa1198ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service endpoint\n",
    "#Atenção ao endpoint, porque alguns modelos estão disponíveis em regiões especificas. \n",
    "endpoint = \"https://inference.generativeai.us-ashburn-1.oci.oraclecloud.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9634f25d-1491-416b-b566-d1366e5e7066",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_oci = ChatOCIGenAI(\n",
    "    model_id=\"ocid1.generativeaimodel.oc1.iad.amaaaaaask7dceyacxqiaijwxalbynhst6oyg4wttzagz3dai4y2rnwh6wrq\", #open ai gpt 4.1 nano\n",
    "    service_endpoint=endpoint,\n",
    "    compartment_id=COMPARTMENT_ID,\n",
    "    provider=\"meta\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 600,\n",
    "        #\"frequency_penalty\": 0,\n",
    "        #\"presence_penalty\": 0,\n",
    "        \"top_p\": 0.75,\n",
    "        \"top_k\": None#\n",
    "        #\"seed\": None\n",
    "    },\n",
    "    auth_type=AUTH_TYPE,\n",
    "    auth_profile=\"DEFAULT\",\n",
    "    auth_file_location='config'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0ec99-31f1-45d7-9ebb-89c31c63f40a",
   "metadata": {},
   "source": [
    "### Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d824c3bb-b330-4bfa-85b4-181ed528d372",
   "metadata": {},
   "source": [
    "Este teste utiliza um llm as a judge para avaliar 3 diferentes respostas candidatas, que serão avaliadas por um modelo gpt 4.1 nano. O modelo fornecerá score, justificativa e deve citar eventuais problemas com a resposta. No final, deve salvar a resposta em formato json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "983f2679-c5bb-4898-ba91-fd88118e46d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reference = \"\"\"\n",
    "Explique como adicionar elementos a uma lista em Python.\n",
    "\"\"\"\n",
    "\n",
    "candidates = {\n",
    "    \"A\": \"Use lista.append(x) para adicionar um item no final.\",\n",
    "    \"B\": \"Você pode usar append ou extend. Também é possível usar insert para colocar em uma posição específica.\",\n",
    "    \"C\": \"Para adicionar itens em Python use lista.push(x).\"\n",
    "}\n",
    "\n",
    "# --- Monta o bloco das candidatas ---\n",
    "candidates_block = \"\\n\".join([f\"{k}: {v}\" for k, v in candidates.items()])\n",
    "\n",
    "# --- Prompt de juiz para o LLM ---\n",
    "judge_prompt = f\"\"\"\n",
    "Você é um avaliador especializado e imparcial. \n",
    "\n",
    "Sua tarefa é avaliar cada resposta candidata com base em:\n",
    "- Correção factual\n",
    "- Completude\n",
    "- Clareza\n",
    "- Adequação ao enunciado\n",
    "- Uso correto da API de Python\n",
    "\n",
    "Retorne a resposta no seguinte formato:\n",
    "[\n",
    "  {{\"candidate\": \"A\", \"score\": 0.0 a 1.0, \"justification\": \"texto curto\", \"issues\": \"issues\"}},\n",
    "]\n",
    "\n",
    "Precisa finalizar corretamente o json\n",
    "Enunciado de referência:\n",
    "{reference}\n",
    "\n",
    "Respostas candidatas:\n",
    "{candidates_block}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c19cedc7-4b54-40c0-b142-8eecf8866b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Raw JSON output do juiz ---\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"candidate\": \"a\",\n",
      "    \"score\": 1.0,\n",
      "    \"justification\": \"resposta correta, clara e completa, usando a api adequada.\",\n",
      "    \"issues\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"candidate\": \"b\",\n",
      "    \"score\": 0.9,\n",
      "    \"justification\": \"resposta correta e completa, incluindo métodos adicionais, clara e adequada ao enunciado.\",\n",
      "    \"issues\": \"\"\n",
      "  },\n",
      "  {\n",
      "    \"candidate\": \"c\",\n",
      "    \"score\": 0.0,\n",
      "    \"justification\": \"incorreta, pois não existe o método 'push' em listas python.\",\n",
      "    \"issues\": \"uso incorreto do método 'push', que não faz parte da api de listas em python.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "--- Como objeto Python ---\n",
      "\n",
      "[{'candidate': 'a', 'score': 1.0, 'justification': 'resposta correta, clara e completa, usando a api adequada.', 'issues': ''}, {'candidate': 'b', 'score': 0.9, 'justification': 'resposta correta e completa, incluindo métodos adicionais, clara e adequada ao enunciado.', 'issues': ''}, {'candidate': 'c', 'score': 0.0, 'justification': \"incorreta, pois não existe o método 'push' em listas python.\", 'issues': \"uso incorreto do método 'push', que não faz parte da api de listas em python.\"}]\n"
     ]
    }
   ],
   "source": [
    "response = llm_oci.invoke([judge_prompt])\n",
    "resposta = response.content.strip().lower()\n",
    "\n",
    "print(\"\\n--- Raw JSON output do juiz ---\\n\")\n",
    "print(resposta)\n",
    "\n",
    "# Converte para Python\n",
    "print(\"\\n--- Como objeto Python ---\\n\")\n",
    "judgments = json.loads(resposta)\n",
    "print(judgments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9682a4-4d27-4cb1-ac27-21e20ce33e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "75f09525-16d9-4598-a157-5fc406183e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d312ee76-8025-4210-982e-bd84a52252e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65812a-32ce-40fa-b6b8-11e7947cf9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_p310_any_x86_64_v1]",
   "language": "python",
   "name": "conda-env-python_p310_any_x86_64_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
