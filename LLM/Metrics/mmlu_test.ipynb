{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f95081ad-34c8-4196-81aa-795fba524cca",
   "metadata": {},
   "source": [
    "### MMLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc9a9f-e2c8-4049-989f-e5d20e72a86b",
   "metadata": {},
   "source": [
    "Até agora tratamos de métodos criados por volta de 2005. Vamos avançar agora uns 15 anos para frente e avaliar um método que foi criado para avaliar os modelos de linguagens iniciais. \n",
    "\n",
    "MMLU significa Massive Multitask Language Understanding e trata se de um benchmark criado para avaliar a capacidade das LLMs em áreas diversas de conhecimento. Consiste de aproximadamente 16 mill questões variadas do tipo multipla escolha, disponibilizadas por meio de um dataset que pode ser baixado do link https://people.eecs.berkeley.edu/~hendrycks/data.tar\n",
    "\n",
    "Ao abrir o arquivo compactado, o leitor perceberá que o dataset está dividido em diversos arquivos .csv, organizados por assunto, o que permite escolher o domínio de assunto a ser avaliado.\n",
    "\n",
    "O repositório original foi criado por Dan Hendrycks e alguns pesquisadores e pode ser consultado no link https://github.com/hendrycks/test\n",
    "Não deixem de consultar também o paper original: https://arxiv.org/pdf/2009.03300\n",
    "Cada teste consiste de uma pergunta, as opções de respostas disponíveis e um gabarito. A idéia então é submeter a pergunta junto com as opções de resposta para que a LLM tente responder. Depois essa resposta deve ser comparada contra o gabarito. As respostas são informadas sempre como opções ‘a’,’b’,’c’, etc\n",
    "O MMLU original foi criado no início de 2021, e muita coisa tem evoluído desde então. Natural que com a evolução dos modelos, eles alcancem scores cada vez maiores neste teste, mas além disso, existe uma preocupação de que os modelos sendo avaliados se utilizem de ferramentas como WebSearch na resposta, sem contar a possibilidade de que o teste pode ser vazado e consumido durante o treinamento de novos modelos de linguagem, o que acaba por contaminar a resposta e o score obtido.\n",
    "\n",
    "Por conta disso, métodos derivados tem sido criados, buscando inibir a possibilidade de utilizar estes testes no treinamento do modelo ou ao menos dificultar este processo. Caberia um livro aqui, mas podemos citar por exemplo alguns substitutos como AGIEval, GPQA, MMLU-Pro. \n",
    "Vamos exercitar o processo de avaliação com MMLU, através do notebook abaixo. Para este exercício vou utilizar 2 serviços do Oracle OCI: OCI Generative AI (para consumir um modelo de LLM hospedado na nuvem Oracle que no caso será o Open AI GPT 4.1 nano) e o serviço OCI Data Science, comentado no início do artigo , para criação do Jupyter Notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "728c9322-9dd7-4e65-93a2-063a00226507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install langchain_community\n",
    "#! pip install oci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3be36f3d-2ae3-49b3-873e-e6d7e4eee8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import oci\n",
    "\n",
    "#Bibliotecas para uso do langchain + OCI Generative AI\n",
    "from langchain_community.chat_models import ChatOCIGenAI\n",
    "from langchain_community.chat_models import oci_generative_ai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d483ed-6435-452f-9832-830896b291a8",
   "metadata": {},
   "source": [
    "### Autenticação OCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "367c4de2-96b2-4fc5-9b85-477df6e95220",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARTMENT_ID = \"ocid1.compartment.oc1..aaaaaaaacuafyhpnjnsp5luoo3dqklsdi2ysobswq3irzu664gl3cjhvcjpa\"\n",
    "AUTH_TYPE = \"API_KEY\" # The authentication type to use, e.g., API_KEY (default), SECURITY_TOKEN, INSTANCE_PRINCIPAL, RESOURCE_PRINCIPAL.\n",
    "\n",
    "# Auth Config\n",
    "CONFIG_PROFILE = \"DEFAULT\"\n",
    "config = oci.config.from_file('config', CONFIG_PROFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c07ca6-6f0c-45eb-ab3a-89212bbae422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73b0fd93-b069-4397-b921-0e02ba4976ab",
   "metadata": {},
   "source": [
    "### Instância OCI Generative AI com modelo gpt 4.1 nano, executado dentro do OCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "085e41c1-d414-4924-89b1-ecaa1198ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service endpoint\n",
    "#Atenção ao endpoint, porque alguns modelos estão disponíveis em regiões especificas. \n",
    "endpoint = \"https://inference.generativeai.us-ashburn-1.oci.oraclecloud.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9634f25d-1491-416b-b566-d1366e5e7066",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_oci = ChatOCIGenAI(\n",
    "    model_id=\"ocid1.generativeaimodel.oc1.iad.amaaaaaask7dceyacxqiaijwxalbynhst6oyg4wttzagz3dai4y2rnwh6wrq\", #open ai gpt 4.1 nano\n",
    "    service_endpoint=endpoint,\n",
    "    compartment_id=COMPARTMENT_ID,\n",
    "    provider=\"meta\",\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 150,\n",
    "        #\"frequency_penalty\": 0,\n",
    "        #\"presence_penalty\": 0,\n",
    "        \"top_p\": 0.75,\n",
    "        \"top_k\": None#\n",
    "        #\"seed\": None\n",
    "    },\n",
    "    auth_type=AUTH_TYPE,\n",
    "    auth_profile=\"DEFAULT\",\n",
    "    auth_file_location='config'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0ec99-31f1-45d7-9ebb-89c31c63f40a",
   "metadata": {},
   "source": [
    "### Teste 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d824c3bb-b330-4bfa-85b4-181ed528d372",
   "metadata": {},
   "source": [
    "Este é um teste bem simples, para exemplificar o método de avaliação. Caso queira rodar contra um dataset do mmlu, pode executar o teste 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "983f2679-c5bb-4898-ba91-fd88118e46d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmlu_dataset = [\n",
    "    {\n",
    "        \"question\": \"Qual destes minerais é necessário à formação de hemoglobina?\",\n",
    "        \"choices\": [\"a) Ferro\", \"b) Cálcio\", \"c) Potássio\", \"d) Sódio\"],\n",
    "        \"answer\": \"a\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Quem escreveu 'Dom Casmurro'?\",\n",
    "        \"choices\": [\"a) Machado de Assis\", \"b) Clarice Lispector\", \"c) José de Alencar\", \"d) Monteiro Lobato\"],\n",
    "        \"answer\": \"a\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19cedc7-4b54-40c0-b142-8eecf8866b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5f9e76ea-f055-4bb1-ac94-0957ad67833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perguntar_ao_modelo_oci(llm, question, choices):\n",
    "    prompt = question + \"\\n\"\n",
    "    for opt in choices:\n",
    "        prompt += opt + \"\\n\"\n",
    "    prompt += \"Responda apenas com a letra da alternativa correta (a, b, c ou d):\"\n",
    "\n",
    "    response = llm.invoke([prompt])\n",
    "    resposta = response.content.strip().lower()\n",
    "    print(resposta)\n",
    "    # Captura apenas 'a', 'b', 'c', ou 'd'\n",
    "    for alt in ['a', 'b', 'c', 'd']:\n",
    "        if resposta.startswith(alt):\n",
    "            return alt\n",
    "    #fallback se o modelo retornar algo inesperado\n",
    "    return \"-\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ff017658-7065-47e4-b7f4-b30b2dca788e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "Pergunta: Qual destes minerais é necessário à formação de hemoglobina?\n",
      "Resposta do modelo: a | Resposta correta: a\n",
      "\n",
      "a\n",
      "Pergunta: Quem escreveu 'Dom Casmurro'?\n",
      "Resposta do modelo: a | Resposta correta: a\n",
      "\n",
      "Acurácia final do modelo: 100.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corretos = 0\n",
    "for item in mmlu_dataset:\n",
    "    resposta_modelo = perguntar_ao_modelo_oci(llm_oci, item['question'], item['choices'])\n",
    "    print(f\"Pergunta: {item['question']}\")\n",
    "    print(f\"Resposta do modelo: {resposta_modelo} | Resposta correta: {item['answer']}\\n\")\n",
    "    if resposta_modelo == item['answer']:\n",
    "        corretos += 1\n",
    "\n",
    "acuracia = corretos / len(mmlu_dataset)\n",
    "print(f\"Acurácia final do modelo: {acuracia:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f05a9a-1ef8-4fe3-a588-8e6132bd20fa",
   "metadata": {},
   "source": [
    "### Teste 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a37077-62ed-411c-ad43-b2825544e365",
   "metadata": {},
   "source": [
    "Para o segundo teste, já utilizando o dataset do mmlu, escolhi apenas um dos arquivos csv por uma questão de praticidade. O tema escolhido foi astronomia (amo desde criança)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1a463a-6b43-46cb-89d4-b7f9940e29f5",
   "metadata": {},
   "source": [
    "Armazenei junto ao notebook uma pasta chamada mmlu_dataset onde tenho todos os datasets organizados por tema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4f6c4189-8ab6-4ea9-be62-f073bb93ac4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 84.21%\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "mmlu_dataset = []\n",
    "\n",
    "with open('mmlu_dataset/astronomy_test.csv', 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for row in reader:\n",
    "        mmlu_dataset.append({\n",
    "            \"question\": row[0],\n",
    "            \"choices\": [\n",
    "                f\"a) {row[1]}\",\n",
    "                f\"b) {row[2]}\",\n",
    "                f\"c) {row[3]}\",\n",
    "                f\"d) {row[4]}\"\n",
    "            ],\n",
    "            \"answer\": row[5].strip().lower()\n",
    "        })\n",
    "\n",
    "\n",
    "def perguntar_ao_modelo_oci(llm, question, choices):\n",
    "    prompt = question + \"\\n\"\n",
    "    for opt in choices:\n",
    "        prompt += opt + \"\\n\"\n",
    "    prompt += \"Responda apenas com a letra da alternativa correta (a, b, c ou d):\"\n",
    "    response = llm.invoke([prompt])\n",
    "    resposta = response.content.strip().lower()\n",
    "    #print(resposta)\n",
    "    for alt in ['a', 'b', 'c', 'd']:\n",
    "        if resposta.startswith(alt):\n",
    "            return alt\n",
    "    return \"-\"\n",
    "\n",
    "def rodar_teste_mmlu_memoria(llm, mmlu_dataset):\n",
    "    resultados = []\n",
    "    for item in mmlu_dataset:\n",
    "        question = item[\"question\"]\n",
    "        choices = item[\"choices\"]\n",
    "        resposta_correta = item[\"answer\"].lower()\n",
    "        resposta_modelo = perguntar_ao_modelo_oci(llm, question, choices)\n",
    "        resultados.append({\n",
    "            'question': question,\n",
    "            'resposta_modelo': resposta_modelo,\n",
    "            'resposta_correta': resposta_correta,\n",
    "            'acerto': resposta_modelo == resposta_correta\n",
    "        })\n",
    "    return resultados\n",
    "\n",
    "resultados = rodar_teste_mmlu_memoria(llm_oci, mmlu_dataset)\n",
    "acertos = sum([1 for r in resultados if r['acerto']])\n",
    "print(f\"Acurácia: {acertos/len(resultados):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "75f09525-16d9-4598-a157-5fc406183e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d312ee76-8025-4210-982e-bd84a52252e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e65812a-32ce-40fa-b6b8-11e7947cf9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python_p310_any_x86_64_v1]",
   "language": "python",
   "name": "conda-env-python_p310_any_x86_64_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
